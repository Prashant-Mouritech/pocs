{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField, SearchField,VectorSearch, VectorSearchProfile, HnswAlgorithmConfiguration\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI and Azure Search credentials\n",
    "SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_version = os.getenv('API_VERSION')\n",
    "azure_openai_embedding_deployment = \"text-embedding-ada-002\"\n",
    "azure_openai_chat_model = \"gpt-35-turbo\"\n",
    "\n",
    "# Constants\n",
    "SEARCH_INDEX_NAME = \"clinical_trial_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def index_documents_to_azure_search(text_chunks):\n",
    "#     \"\"\"Generates embeddings and indexes chunks of text into Azure Cognitive Search.\"\"\"\n",
    "#     openai_credential = DefaultAzureCredential()\n",
    "    \n",
    "#     client = AzureOpenAI(\n",
    "#         azure_deployment=azure_openai_embedding_deployment,\n",
    "#         azure_endpoint=azure_openai_endpoint,\n",
    "#         api_key=azure_openai_key,\n",
    "#         azure_ad_token_provider=openai_credential if not azure_openai_key else None,\n",
    "#         api_version=azure_openai_version\n",
    "#     )\n",
    "\n",
    "#     client_search = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=SEARCH_INDEX_NAME, credential=AzureKeyCredential(SEARCH_ADMIN_KEY))\n",
    "\n",
    "#     for i, chunk in enumerate(text_chunks):\n",
    "#         document = {\n",
    "#             \"id\": str(i),\n",
    "#             \"fileName\": \"DNDi-Clinical-Trial-Protocol-BENDITA-V5.pdf\",  # Replace this with the actual file name\n",
    "#             \"content\": chunk,\n",
    "#             \"contentEmbeddings\": []  # Placeholder for embeddings\n",
    "#         }\n",
    "\n",
    "#         # Create embeddings for the content chunk\n",
    "#         try:\n",
    "#             response = client.embeddings.create(input=chunk, model=\"text-embedding-ada-002\")\n",
    "#             embeddings = response.data[0].embedding\n",
    "#             document[\"contentEmbeddings\"] = embeddings  # Store the embeddings\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating embeddings for chunk {i}: {e}\")\n",
    "\n",
    "#         # Upload the document with embeddings to Azure Search\n",
    "#         try:\n",
    "#             client_search.upload_documents(documents=[document])\n",
    "#             print(f\"Document chunk '{document['id']}' indexed successfully.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to index document chunk '{document['id']}': {e}\")\n",
    "####################################################################################################\n",
    "def search_embeddings(user_question):\n",
    "    client_search = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=SEARCH_INDEX_NAME, credential=AzureKeyCredential(SEARCH_ADMIN_KEY))\n",
    "    # Generate embeddings for the user's question using Azure OpenAI\n",
    "    try:\n",
    "        openai_credential = AzureKeyCredential(azure_openai_key)\n",
    "        client = AzureOpenAI(\n",
    "            azure_deployment=azure_openai_embedding_deployment,\n",
    "            azure_endpoint=azure_openai_endpoint,\n",
    "            api_key=azure_openai_key,\n",
    "            api_version=azure_openai_version,\n",
    "        )\n",
    "        response = client.embeddings.create(input=user_question, model=\"text-embedding-ada-002\").data[0].embedding\n",
    "\n",
    "        vector_query = VectorizedQuery(vector=response, k_nearest_neighbors=3, fields=\"contentEmbeddings\", exhaustive=True)\n",
    "        # print(\"This is extracted vector\",vector_query)\n",
    "        results = client_search.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"content\"],\n",
    "            top=3\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings or searching: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_chat_response(user_question):\n",
    "    try:\n",
    "        # First, search the embeddings using the user's question\n",
    "        search_results = search_embeddings(user_question)\n",
    "        \n",
    "        # Prepare a system prompt based on search results (can be customized)\n",
    "        prompt = \"\"\"\n",
    "                Answer the question as detailed as possible from the provided context, make sure to provide all the details. \n",
    "                If the answer is not in the provided context, just say, \"answer is not available in the context\", don't provide the wrong answer.\\n\\n\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "\n",
    "        for result in search_results:\n",
    "            prompt += f\"{result['content']}\\n\\n\"\n",
    "        \n",
    "        # Add the user's question to the prompt\n",
    "        prompt += f\"User question: {user_question}\\n\\n\"\n",
    "        \n",
    "        # Use GPT-35-turbo for chat completion\n",
    "        try:\n",
    "            client = AzureOpenAI(\n",
    "                azure_deployment=azure_openai_chat_model,\n",
    "                azure_endpoint=azure_openai_endpoint,\n",
    "                api_key=azure_openai_key,\n",
    "                api_version=azure_openai_version,\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-35-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "            )\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            # Return the response generated by GPT-35-turbo\n",
    "            return {\n",
    "                \"response\": output,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Screening criteria for the trial include: \\n- Signed, written informed consent form  \\n- Age between 18 and 50 years \\n- Weight between 50kg and 80kg \\n- Diagnosis of T. cruzi infection \\n- Ability to comply with all protocol specified tests and visits and have a permanent address \\n- Residents of areas free of vectorial transmission \\n- No signs and/or symptoms of the chronic cardiac and/or digestive form of CD\\n- No acute or chronic health conditions that, in the opinion of the principal investigator, may prevent participation in the trial.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chat_response(\"Screening criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(user_question):\n",
    "    client_search = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=SEARCH_INDEX_NAME, credential=AzureKeyCredential(SEARCH_ADMIN_KEY))\n",
    "    # Generate embeddings for the user's question using Azure OpenAI\n",
    "    try:\n",
    "        openai_credential = AzureKeyCredential(azure_openai_key)\n",
    "        client = AzureOpenAI(\n",
    "            azure_deployment=azure_openai_embedding_deployment,\n",
    "            azure_endpoint=azure_openai_endpoint,\n",
    "            api_key=azure_openai_key,\n",
    "            api_version=azure_openai_version,\n",
    "        )\n",
    "        response = client.embeddings.create(input=user_question, model=\"text-embedding-ada-002\").data[0].embedding\n",
    "\n",
    "        vector_query = VectorizedQuery(vector=response, k_nearest_neighbors=1, fields=\"contentEmbeddings\", exhaustive=True)\n",
    "\n",
    "        \n",
    "        # Perform the search using vector search\n",
    "        search_results = client_search.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"content\"],\n",
    "            top=1\n",
    "        )\n",
    "\n",
    "        # Collect the results into a list (since SearchItemPaged is an iterable)\n",
    "        # results_list = [result for result in search_results]\n",
    "        return search_results #results_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings or searching: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_response(user_question):\n",
    "    try:\n",
    "        # First, search the embeddings using the user's question\n",
    "        search_results = search_embeddings(user_question)\n",
    "\n",
    "        \n",
    "\n",
    "        # Prepare a system prompt based on search result\n",
    "        prompt = \"\"\"\n",
    "                Answer the question as detailed as possible from the provided context,\n",
    "                make sure to provide all the details. If the answer is not in the provided context, \n",
    "                just say, \"answer is not available in the context\", don't provide the wrong answer.\\n\\n\n",
    "                Context:\\n\n",
    "                \"\"\"\n",
    "\n",
    "        # Since there is only one result, directly access the first result\n",
    "        prompt += f\"{search_results['content']}\\n\\n\"\n",
    "\n",
    "        # Add the user's question to the prompt\n",
    "        prompt += f\"User question: {user_question}\\n\\n\"\n",
    "\n",
    "        # Use GPT-35-turbo for chat completion\n",
    "        try:\n",
    "            client = AzureOpenAI(\n",
    "                azure_deployment=azure_openai_chat_model,\n",
    "                azure_endpoint=azure_openai_endpoint,\n",
    "                api_key=azure_openai_key,\n",
    "                api_version=azure_openai_version,\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-35-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "            )\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            # Return the response generated by GPT-35-turbo\n",
    "            return {\n",
    "                \"response\": output,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "'SearchItemPaged' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mget_chat_response\u001b[1;34m(user_question)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Since there is only one result, directly access the first result\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Add the user's question to the prompt\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SearchItemPaged' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScreening criteria\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m, in \u001b[0;36mget_chat_response\u001b[1;34m(user_question)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError generating response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[1;31mException\u001b[0m: 'SearchItemPaged' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "get_chat_response(\"Screening criteria\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
